@misc{Standarization,
title={Standarization},
howpublished={\url{https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html}}}
@misc{credit,
  author       = {Yeh,I-Cheng},
  title        = {{default of credit card clients}},
  year         = {2016},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C55S3H}
}

@article{xu2021traversing,
  title={Traversing the local polytopes of relu neural networks: A unified approach for network verification},
  author={Xu, Shaojie and Vaughan, Joel and Chen, Jie and Zhang, Aijun and Sudjianto, Agus},
  journal={arXiv preprint arXiv:2111.08922},
  year={2021}
}
@inproceedings{serra2018bounding,
  title={Bounding and counting linear regions of deep neural networks},
  author={Serra, Thiago and Tjandraatmadja, Christian and Ramalingam, Srikumar},
  booktitle={International Conference on Machine Learning},
  pages={4558--4566},
  year={2018},
  organization={PMLR}
}
@inproceedings{croce2019provable,
  title={Provable robustness of relu networks via maximization of linear regions},
  author={Croce, Francesco and Andriushchenko, Maksym and Hein, Matthias},
  booktitle={the 22nd International Conference on Artificial Intelligence and Statistics},
  pages={2057--2066},
  year={2019},
  organization={PMLR}
}
@article{robinson2019dissecting,
  title={Dissecting deep neural networks},
  author={Robinson, Haakon and Rasheed, Adil and San, Omer},
  journal={arXiv preprint arXiv:1910.03879},
  year={2019}
}
@article{hanin2019deep,
  title={Deep relu networks have surprisingly few activation patterns},
  author={Hanin, Boris and Rolnick, David},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{sattelberg2023locally,
  title={Locally linear attributes of relu neural networks},
  author={Sattelberg, Ben and Cavalieri, Renzo and Kirby, Michael and Peterson, Chris and Beveridge, Ross},
  journal={Frontiers in Artificial Intelligence},
  volume={6},
  year={2023},
  publisher={Frontiers Media SA}
}

@misc{crime,
  author       = {Redmond,Michael},
  title        = {{Communities and Crime}},
  year         = {2009},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C53W3X}
}
@misc{German,
  author       = {Hofmann,Hans},
  title        = {{Statlog (German Credit Data)}},
  year         = {1994},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C5NC77}
}

@misc{Adult,
  author       = {Becker,Barry and Kohavi,Ronny},
  title        = {{Adult}},
  year         = {1996},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C5XW20}
}

@software{gnark-v0.9.0,
  author       = {Gautam Botrel and
                  Thomas Piellard and
                  Youssef El Housni and
                  Ivo Kubjas and
                  Arya Tabaie},
  title        = {ConsenSys/gnark: v0.9.0},
  month        = feb,
  year         = 2023,
  publisher    = {Zenodo},
  version      = {v0.9.0},
  doi          = {10.5281/zenodo.5819104},
  url          = {https://doi.org/10.5281/zenodo.5819104}
}
@book{barocas-hardt-narayanan,
  title = {Fairness and Machine Learning: Limitations and Opportunities},
  author = {Solon Barocas and Moritz Hardt and Arvind Narayanan},
  publisher = {fairmlbook.org},
  note = {\url{http://www.fairmlbook.org}},
  year = {2019}
}
@article{GMW,
author = {Goldreich, Oded and Micali, Silvio and Wigderson, Avi},
title = {Proofs That Yield Nothing but Their Validity or All Languages in NP Have Zero-Knowledge Proof Systems},
year = {1991},
issue_date = {July 1991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {3},
issn = {0004-5411},
url = {https://doi.org/10.1145/116825.116852},
doi = {10.1145/116825.116852},
journal = {J. ACM},
month = {jul},
pages = {690–728},
numpages = {39},
keywords = {NP, zero-knowledge, proof systems, methodological design of protocols, one-way functions, interactive proofs, graph isomorphism, cryptographic protocols, fault tolerant distributed computing}
}

@misc{sun2023zkdl,
      title={zkDL: Efficient Zero-Knowledge Proofs of Deep Learning Training}, 
      author={Haochen Sun and Hongyang Zhang},
      year={2023},
      eprint={2307.16273},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{vCNN,
  title={vCNN: Verifiable Convolutional Neural Network},
  author={Seunghwan Lee and Hankyung Ko and Jihye Kim and Hyunok Oh},
  journal={IACR Cryptol. ePrint Arch.},
  year={2020},
  volume={2020},
  pages={584},
  url={https://api.semanticscholar.org/CorpusID:218895602}
}
@article{Zen,
  title={ZEN: Efficient Zero-Knowledge Proofs for Neural Networks},
  author={Boyuan Feng and Lianke Qin and Zhenfei Zhang and Yufei Ding and Shumo Chu},
  journal={IACR Cryptol. ePrint Arch.},
  year={2021},
  volume={2021},
  pages={87},
  url={https://api.semanticscholar.org/CorpusID:231731893}
}
@article{survey,
author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
title = {A Survey on Bias and Fairness in Machine Learning},
year = {2021},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3457607},
doi = {10.1145/3457607},
abstract = {With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {115},
numpages = {35},
keywords = {machine learning, representation learning, deep learning, natural language processing, Fairness and bias in artificial intelligence}
}




@inproceedings{DemographicParity,
author = {Kusner, Matt and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
title = {Counterfactual Fairness},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {4069–4079},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}
@inproceedings{
Yurochkin2020Training,
title={Training individually fair ML models with sensitive subspace robustness},
author={Mikhail Yurochkin and Amanda Bower and Yuekai Sun},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=B1gdkxHFDH}
}
@article{Bastani2019,
author = {Bastani, Osbert and Zhang, Xin and Solar-Lezama, Armando},
title = {Probabilistic Verification of Fairness Properties via Concentration},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {OOPSLA},
url = {https://doi.org/10.1145/3360544},
doi = {10.1145/3360544},
abstract = {As machine learning systems are increasingly used to make real world legal and financial decisions, it is of paramount importance that we develop algorithms to verify that these systems do not discriminate against minorities. We design a scalable algorithm for verifying fairness specifications. Our algorithm obtains strong correctness guarantees based on adaptive concentration inequalities; such inequalities enable our algorithm to adaptively take samples until it has enough data to make a decision. We implement our algorithm in a tool called VeriFair, and show that it scales to large machine learning models, including a deep recurrent neural network that is more than five orders of magnitude larger than the largest previously-verified neural network. While our technique only gives probabilistic guarantees due to the use of random samples, we show that we can choose the probability of error to be extremely small.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {118},
numpages = {27},
keywords = {fairness, machine learning, probabilistic verification}
}
@misc{doherty2023individual,
      title={Individual Fairness in Bayesian Neural Networks}, 
      author={Alice Doherty and Matthew Wicker and Luca Laurenti and Andrea Patane},
      year={2023},
      eprint={2304.10828},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{CertiFair,
author = {Khedr, Haitham and Shoukry, Yasser},
title = {CertiFair: A Framework for Certified Global Fairness of Neural Networks},
year = {2023},
isbn = {978-1-57735-880-0},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v37i7.25994},
doi = {10.1609/aaai.v37i7.25994},
abstract = {We consider the problem of whether a Neural Network (NN) model satisfies global individual fairness. Individual Fairness (defined in (Dwork et al. 2012)) suggests that <i>similar</i> individuals with respect to a certain task are to be treated <i>similarly</i> by the decision model. In this work, we have two main objectives. The first is to construct a verifier which checks whether the fairness property holds for a given NN in a classification task or provides a counterexample if it is violated, i.e., the model is fair if all similar individuals are classified the same, and unfair if a pair of similar individuals are classified differently. To that end, we construct a sound and complete verifier that verifies global individual fairness properties of ReLU NN classifiers using distance-based similarity metrics. The second objective of this paper is to provide a method for training provably fair NN classifiers from unfair (biased) data. We propose a fairness loss that can be used during training to enforce fair outcomes for similar individuals. We then provide provable bounds on the fairness of the resulting NN. We run experiments on commonly used fairness datasets that are publicly available and we show that global individual fairness can be improved by 96 \% without a significant drop in test accuracy.},
booktitle = {Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {925},
numpages = {9},
series = {AAAI'23/IAAI'23/EAAI'23}
}
@inproceedings{Benussi2022IndividualFG,
  title={Individual Fairness Guarantees for Neural Networks},
  author={Elias Benussi and Andrea Patan{\'e} and Matthew Wicker and Luca Laurenti and Marta Kwiatkowska University of Oxford and Tu Delft},
  booktitle={International Joint Conference on Artificial Intelligence},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:248722046}
}
@article{Urban2020,
author = {Urban, Caterina and Christakis, Maria and W\"{u}stholz, Valentin and Zhang, Fuyuan},
title = {Perfectly Parallel Fairness Certification of Neural Networks},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {OOPSLA},
url = {https://doi.org/10.1145/3428253},
doi = {10.1145/3428253},
abstract = {Recently, there is growing concern that machine-learned software, which currently assists or even automates decision making, reproduces, and in the worst case reinforces, bias present in the training data. The development of tools and techniques for certifying fairness of this software or describing its biases is, therefore, critical. In this paper, we propose a perfectly parallel static analysis for certifying fairness of feed-forward neural networks used for classification of tabular data. When certification succeeds, our approach provides definite guarantees, otherwise, it describes and quantifies the biased input space regions. We design the analysis to be sound, in practice also exact, and configurable in terms of scalability and precision, thereby enabling pay-as-you-go certification. We implement our approach in an open-source tool called Libra and demonstrate its effectiveness on neural networks trained on popular datasets.},
journal = {Proc. ACM Program. Lang.},
month = {nov},
articleno = {185},
numpages = {30},
keywords = {Neural Networks, Static Analysis, Abstract Interpretation, Fairness}
}

@misc{john2020verifying,
      title={Verifying Individual Fairness in Machine Learning Models}, 
      author={Philips George John and Deepak Vijaykeerthy and Diptikalyan Saha},
      year={2020},
      eprint={2006.11737},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{Ruoss2020,
 author = {Ruoss, Anian and Balunovic, Mislav and Fischer, Marc and Vechev, Martin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {7584--7596},
 publisher = {Curran Associates, Inc.},
 title = {Learning Certified Individually Fair Representations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/55d491cf951b1b920900684d71419282-Paper.pdf},
 volume = {33},
 year = {2020}
}
@INPROCEEDINGS{Fairify,
  author={Biswas, Sumon and Rajan, Hridesh},
  booktitle={2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)}, 
  title={Fairify: Fairness Verification of Neural Networks}, 
  year={2023},
  volume={},
  number={},
  pages={1546-1558},
  doi={10.1109/ICSE48619.2023.00134}}


@misc{yan2022active,
      title={Active Fairness Auditing}, 
      author={Tom Yan and Chicheng Zhang},
      year={2022},
      eprint={2206.08450},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{AVOIR,
author = {Maneriker, Pranav and Burley, Codi and Parthasarathy, Srinivasan},
title = {Online Fairness Auditing through Iterative Refinement},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599454},
doi = {10.1145/3580305.3599454},
abstract = {A sizable proportion of deployed machine learning models make their decisions in a black-box manner. Such decision-making procedures are susceptible to intrinsic biases, which has led to a call for accountability in deployed decision systems. In this work, we investigate mechanisms that help audit claimed mathematical guarantees of the fairness of such systems. We construct AVOIR, a system that reduces the number of observations required for the runtime monitoring of probabilistic assertions over fairness metrics specified on decision functions associated with black-box AI models. AVOIR provides an adaptive process that automates the inference of probabilistic guarantees associated with estimating a wide range of fairness metrics. In addition, AVOIR enables the exploration of fairness violations aligned with governance and regulatory requirements. We conduct case studies with fairness metrics on three different datasets and demonstrate how AVOIR can help detect and localize fairness violations and ameliorate the issues with faulty fairness metric design.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {1665–1676},
numpages = {12},
keywords = {monitoring, fairness, online, inference, verification, metrics},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@misc{VI1,
title={},
howpublished={\url{}},
author={},
year={}}

@misc{kang2022scaling,
      title={Scaling up Trustless DNN Inference with Zero-Knowledge Proofs}, 
      author={Daniel Kang and Tatsunori Hashimoto and Ion Stoica and Yi Sun},
      year={2022},
      eprint={2210.08674},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}
@article{PvCNN,
author = {Weng, Jiasi and Weng, Jian and Tang, Gui and Yang, Anjia and Li, Ming and Liu, Jia-Nan},
title = {PvCNN: Privacy-Preserving and Verifiable Convolutional Neural Network Testing},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {18},
issn = {1556-6013},
url = {https://doi.org/10.1109/TIFS.2023.3262932},
doi = {10.1109/TIFS.2023.3262932},
abstract = {We propose a new approach for privacy-preserving and verifiable convolutional neural network (CNN) testing in a distrustful multi-stakeholder environment. The approach is aimed to enable that a CNN model developer convinces a user of the truthful CNN performance over non-public data from multiple testers, while respecting model and data privacy. To balance the security and efficiency issues, we appropriately integrate three tools with the CNN testing, including collaborative inference, homomorphic encryption (HE) and zero-knowledge succinct non-interactive argument of knowledge (zk-SNARK). We start with strategically partitioning a CNN model into a private part kept locally by the model developer, and a public part outsourced to an outside server. Then, the private part runs over the HE-protected test data sent by a tester, and transmits its outputs to the public part for accomplishing subsequent computations of the CNN testing. Second, the correctness of the above CNN testing is enforced by generating zk-SNARK based proofs, with an emphasis on optimizing proving overhead for two-dimensional (2-D) convolution operations, since the operations dominate the performance bottleneck during generating proofs. We specifically present a new quadratic matrix program (QMP)-based arithmetic circuit with a single multiplication gate for expressing 2-D convolution operations between multiple filters and inputs in a batch manner. Third, we aggregate multiple proofs with respect to a same CNN model but different testers’ test data (i.e., different statements) into one proof, and ensure that the validity of the aggregated proof implies the validity of the original multiple proofs. Lastly, our experimental results demonstrate that our QMP-based zk-SNARK performs nearly <inline-formula> <tex-math notation="LaTeX">$13.9times $ </tex-math></inline-formula> faster than the existing quadratic arithmetic program (QAP)-based zk-SNARK in proving time, and <inline-formula> <tex-math notation="LaTeX">$17.6times $ </tex-math></inline-formula> faster in Setup time, for high-dimension matrix multiplication. Besides, the limitation on handling a bounded number of multiplications of QAP-based zk-SNARK is relieved.},
journal = {Trans. Info. For. Sec.},
month = {mar},
pages = {2218–2233},
numpages = {16}
}
@misc{VI2,
title={Zator: Verified inference of a 512-layer neural network using recursive SNARKs},
howpublished={\url{https://github.com/lyronctk/zator/tree/main}},
year={2023}}

@misc{PoT,
      author = {Sanjam Garg and Aarushi Goel and Somesh Jha and Saeed Mahloujifar and Mohammad Mahmoody and Guru-Vamsi Policharla and Mingyuan Wang},
      title = {Experimenting with Zero-Knowledge Proofs of Training},
      howpublished = {Cryptology ePrint Archive, Paper 2023/1345},
      year = {2023},
      note = {\url{https://eprint.iacr.org/2023/1345}},
      url = {https://eprint.iacr.org/2023/1345}
}
@ARTICLE{VeriML,
  author={Zhao, Lingchen and Wang, Qian and Wang, Cong and Li, Qi and Shen, Chao and Feng, Bo},
  journal={IEEE Transactions on Parallel and Distributed Systems}, 
  title={VeriML: Enabling Integrity Assurances and Fair Payments for Machine Learning as a Service}, 
  year={2021},
  volume={32},
  number={10},
  pages={2524-2540},
  doi={10.1109/TPDS.2021.3068195}}



@inproceedings{ZKDT,
author = {Zhang, Jiaheng and Fang, Zhiyong and Zhang, Yupeng and Song, Dawn},
title = {Zero Knowledge Proofs for Decision Tree Predictions and Accuracy},
year = {2020},
isbn = {9781450370899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372297.3417278},
doi = {10.1145/3372297.3417278},
abstract = {Machine learning has become increasingly prominent and is widely used in various applications in practice. Despite its great success, the integrity of machine learning predictions and accuracy is a rising concern. The reproducibility of machine learning models that are claimed to achieve high accuracy remains challenging, and the correctness and consistency of machine learning predictions in real products lack any security guarantees. In this paper, we initiate the study of zero knowledge machine learning and propose protocols for zero knowledge decision tree predictions and accuracy tests. The protocols allow the owner of a decision tree model to convince others that the model computes a prediction on a data sample, or achieves a certain accuracy on a public dataset, without leaking any information about the model itself. We develop approaches to efficiently turn decision tree predictions and accuracy into statements of zero knowledge proofs. We implement our protocols and demonstrate their efficiency in practice. For a decision tree model with 23 levels and 1,029 nodes, it only takes 250 seconds to generate a zero knowledge proof proving that the model achieves high accuracy on a dataset of 5,000 samples and 54 attributes, and the proof size is around 287 kilobytes.},
booktitle = {Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2039–2053},
numpages = {15},
keywords = {zero knowledge proofs, machine learning, decision tree},
location = {Virtual Event, USA},
series = {CCS '20}
}
@article{Liu2021zkCNNZK,
  title={zkCNN: Zero Knowledge Proofs for Convolutional Neural Network Predictions and Accuracy},
  author={Tianyi Liu and Xiang Xie and Yupeng Zhang},
  journal={Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:235349006}
}
@inproceedings{Ghosh2020JusticiaAS,
  title={Justicia: A Stochastic SAT Approach to Formally Verify Fairness},
  author={Bishwamittra Ghosh and D. Basu and Kuldeep S. Meel},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:221655566}
}
@article{FairSquare,
author = {Albarghouthi, Aws and D'Antoni, Loris and Drews, Samuel and Nori, Aditya V.},
title = {FairSquare: Probabilistic Verification of Program Fairness},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {OOPSLA},
url = {https://doi.org/10.1145/3133904},
doi = {10.1145/3133904},
abstract = {With the range and sensitivity of algorithmic decisions expanding at a break-neck speed, it is imperative that we aggressively investigate fairness and bias in decision-making programs. First, we show that a number of recently proposed formal definitions of fairness can be encoded as probabilistic program properties. Second, with the goal of enabling rigorous reasoning about fairness, we design a novel technique for verifying probabilistic properties that admits a wide class of decision-making programs. Third, we present FairSquare, the first verification tool for automatically certifying that a program meets a given fairness property. We evaluate FairSquare on a range of decision-making programs. Our evaluation demonstrates FairSquare’s ability to verify fairness for a range of different programs, which we show are out-of-reach for state-of-the-art program analysis techniques.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {80},
numpages = {30},
keywords = {Probabilistic Inference, Algorithmic Fairness, Probabilistic Programming}
}
@inproceedings{smoothing,
author = {Yeom, Samuel and Fredrikson, Matt},
title = {Individual Fairness Revisited: Transferring Techniques from Adversarial Robustness},
year = {2021},
isbn = {9780999241165},
abstract = {We turn the definition of individual fairness on its head--rather than ascertaining the fairness of a model given a predetermined metric, we find a metric for a given model that satisfies individual fairness. This can facilitate the discussion on the fairness of a model, addressing the issue that it may be difficult to specify a priori a suitable metric. Our contributions are twofold: First, we introduce the definition of a minimal metric and characterize the behavior of models in terms of minimal metrics. Second, for more complicated models, we apply the mechanism of randomized smoothing from adversarial robustness to make them individually fair under a given weighted Lp metric. Our experiments show that adapting the minimal metrics of linear models to more complicated neural networks can lead to meaningful and interpretable fairness guarantees at little cost to utility.},
booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
articleno = {61},
numpages = {7},
location = {Yokohama, Yokohama, Japan},
series = {IJCAI'20}
}
@misc{pentyala2022privfair,
      title={PrivFair: a Library for Privacy-Preserving Fairness Auditing}, 
      author={Sikha Pentyala and David Melanson and Martine De Cock and Golnoosh Farnadi},
      year={2022},
      eprint={2202.04058},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{Toreini2023VerifiableFP,
  title={Verifiable Fairness: Privacy-preserving Computation of Fairness for Machine Learning Systems},
  author={Ehsan Toreini and Maryam Mehrnezhad and Aad van Moorsel},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:261696588}
}


@inproceedings{Park22,
author = {Park, Saerom and Kim, Seongmin and Lim, Yeon-sup},
title = {Fairness Audit of Machine Learning Models with Confidential Computing},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512244},
doi = {10.1145/3485447.3512244},
abstract = {Algorithmic discrimination is one of the significant concerns in applying machine learning models to a real-world system. Many researchers have focused on developing fair machine learning algorithms without discrimination based on legally protected attributes. However, the existing research has barely explored various security issues that can occur while evaluating model fairness and verifying fair models. In this study, we propose a fairness audit framework that assesses the fairness of ML algorithms while addressing potential security issues such as data privacy, model secrecy, and trustworthiness. To this end, our proposed framework utilizes confidential computing and builds a chain of trust through enclave attestation primitives combined with public scrutiny and state-of-the-art software-based security techniques, enabling fair ML models to be securely certified and clients to verify a certified one. Our micro-benchmarks on various ML models and real-world datasets show the feasibility of the fairness certification implemented with Intel SGX in practice. In addition, we analyze the impact of data poisoning, which is an additional threat during data collection for fairness auditing. Based on the analysis, we illustrate the theoretical curves of fairness gap and minimal group size and the empirical results of fairness certification on poisoned datasets.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {3488–3499},
numpages = {12},
keywords = {Fairness, Confidential computing, Security and privacy, Algorithmic audit},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}
@inproceedings{Segal21,
author = {Segal, Shahar and Adi, Yossi and Pinkas, Benny and Baum, Carsten and Ganesh, Chaya and Keshet, Joseph},
title = {Fairness in the Eyes of the Data: Certifying Machine-Learning Models},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462554},
doi = {10.1145/3461702.3462554},
abstract = {We present a framework that allows to certify the fairness degree of a model based on an interactive and privacy-preserving test. The framework verifies any trained model, regardless of its training process and architecture. Thus, it allows us to evaluate any deep learning model on multiple fairness definitions empirically. We tackle two scenarios, where either the test data is privately available only to the tester or is publicly known in advance, even to the model creator. We investigate the soundness of the proposed approach using theoretical analysis and present statistical guarantees for the interactive test. Finally, we provide a cryptographic technique to automate fairness testing and certified inference with only black-box access to the model at hand while hiding the participants' sensitive data.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {926–935},
numpages = {10},
keywords = {fairness, machine-learning, privacy, cryptography},
location = {Virtual Event, USA},
series = {AIES '21}
}
@InProceedings{BlindJustice,
  title = 	 {Blind Justice: Fairness with Encrypted Sensitive Attributes},
  author =       {Kilbertus, Niki and Gascon, Adria and Kusner, Matt and Veale, Michael and Gummadi, Krishna and Weller, Adrian},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2630--2639},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/kilbertus18a/kilbertus18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/kilbertus18a.html},
  abstract = 	 {Recent work has explored how to train machine learning models which do not discriminate against any subgroup of the population as determined by sensitive attributes such as gender or race. To avoid disparate treatment, sensitive attributes should not be considered. On the other hand, in order to avoid disparate impact, sensitive attributes must be examined, e.g., in order to learn a fair model, or to check if a given model is fair. We introduce methods from secure multi-party computation which allow us to avoid both. By encrypting sensitive attributes, we show how an outcome-based fair model may be learned, checked, or have its outputs verified and held to account, without users revealing their sensitive attributes.}
}

@article{confidant,
title={CONFIDENTIAL PROOF OF FAIR TRAINING OF TREES},
journal={ICLR},
author={Ali Shahin Shamsabadi and Sierra Calanda Wyllie and Nicholas Franzese and Natalie Dullerud and Sébastien Gambs and Nicolas Papernot and Xiao Wang and Adrian Weller},
year={2023}}
@misc{fukuchi2019faking,
      title={Faking Fairness via Stealthily Biased Sampling}, 
      author={Kazuto Fukuchi and Satoshi Hara and Takanori Maehara},
      year={2019},
      eprint={1901.08291},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@misc{finlegal,
title={
Right to Financial Privacy Act},
howpublished={\url{https://epic.org/the-right-to-financial-privacy-act/}},
publisher={epic}
}

@inproceedings{robustfair1,
author = {Ruoss, Anian and Balunovi\'{c}, Mislav and Fischer, Marc and Vechev, Martin},
title = {Learning Certified Individually Fair Representations},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Fair representation learning provides an effective way of enforcing fairness constraints without compromising utility for downstream users. A desirable family of such fairness constraints, each requiring similar treatment for similar individuals, is known as individual fairness. In this work, we introduce the first method that enables data consumers to obtain certificates of individual fairness for existing and new data points. The key idea is to map similar individuals to close latent representations and leverage this latent proximity to certify individual fairness. That is, our method enables the data producer to learn and certify a representation where for a data point all similar individuals are at ℓ∞ -distance at most ε, thus allowing data consumers to certify individual fairness by proving ε-robustness of their classifier. Our experimental evaluation on five real-world datasets and several fairness constraints demonstrates the expressivity and scalability of our approach.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {636},
numpages = {13},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}




@article{IF,
Author = {Bertrand, Marianne and Mullainathan, Sendhil},
Title = {Are Emily and Greg More Employable Than Lakisha and Jamal? A Field Experiment on Labor Market Discrimination},
Journal = {American Economic Review},
Volume = {94},
Number = {4},
Year = {2004},
Month = {September},
Pages = {991-1013},
DOI = {10.1257/0002828042002561},
URL = {https://www.aeaweb.org/articles?id=10.1257/0002828042002561}}



@misc{AppleCard,
author={Vigdor, N},
title={Apple Card Investigated After Gender Discrimination Complaints.},
publisher={The New York Times},
year={November,
2019}}



@misc{jobunfair,
publisher={MIT Technology Review},
title={LinkedIn’s job-matching AI was biased. The company’s solution? More AI.},
author={Sheridan Wallarchive and Hilke Schellmannarchive},
year={June, 2021}}

@misc{khedr2022certifair,
      title={CertiFair: A Framework for Certified Global Fairness of Neural Networks}, 
      author={Haitham Khedr and Yasser Shoukry},
      year={2022},
      eprint={2205.09927},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{creditprediction,
title = {Consumer credit-risk models via machine-learning algorithms},
journal = {Journal of Banking \& Finance},
volume = {34},
number = {11},
pages = {2767-2787},
year = {2010},
issn = {0378-4266},
doi = {https://doi.org/10.1016/j.jbankfin.2010.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0378426610002372},
author = {Amir E. Khandani and Adlar J. Kim and Andrew W. Lo},
keywords = {Household behavior, Consumer credit risk, Credit card borrowing, Machine learning, Nonparametric estimation},
abstract = {We apply machine-learning techniques to construct nonlinear nonparametric forecasting models of consumer credit risk. By combining customer transactions and credit bureau data from January 2005 to April 2009 for a sample of a major commercial bank’s customers, we are able to construct out-of-sample forecasts that significantly improve the classification rates of credit-card-holder delinquencies and defaults, with linear regression R2’s of forecasted/realized delinquencies of 85%. Using conservative assumptions for the costs and benefits of cutting credit lines based on machine-learning forecasts, we estimate the cost savings to range from 6% to 25% of total losses. Moreover, the time-series patterns of estimated delinquency rates from this model over the course of the recent financial crisis suggest that aggregated consumer credit-risk analytics may have important applications in forecasting systemic risk.}
}
@misc{unfair1,
author={Julia Angwin and Jeff Larson and Surya Mattu and Lauren Kirchner},
title={Machine Bias},
howpublished={\url{https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing}},
year={2016}
}

@misc{NYClaw,
title={https://www.stroock.com/news-and-insights/are-you-ready-for-nycs-anti-bias-ai-law},
howpublished={\url{https://www.stroock.com/news-and-insights/are-you-ready-for-nycs-anti-bias-ai-law}},
year={2023}}



@misc{resume,
author={Dastin, J}, 
title={Amazon scraps secret AI recruiting tool that
showed bias against women},
publisher={Reuters}, 
year={October 2018}}

@article{Adfair,
author = {Ali, Muhammad and Sapiezynski, Piotr and Bogen, Miranda and Korolova, Aleksandra and Mislove, Alan and Rieke, Aaron},
title = {Discrimination through Optimization: How Facebook's Ad Delivery Can Lead to Biased Outcomes},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359301},
doi = {10.1145/3359301},
abstract = {The enormous financial success of online advertising platforms is partially due to the precise targeting features they offer. Although researchers and journalists have found many ways that advertisers can target---or exclude---particular groups of users seeing their ads, comparatively little attention has been paid to the implications of the platform's ad delivery process, comprised of the platform's choices about which users see which ads. It has been hypothesized that this process can "skew" ad delivery in ways that the advertisers do not intend, making some users less likely than others to see particular ads based on their demographic characteristics. In this paper, we demonstrate that such skewed delivery occurs on Facebook, due to market and financial optimization effects as well as the platform's own predictions about the "relevance" of ads to different groups of users. We find that both the advertiser's budget and the content of the ad each significantly contribute to the skew of Facebook's ad delivery. Critically, we observe significant skew in delivery along gender and racial lines for "real" ads for employment and housing opportunities despite neutral targeting parameters. Our results demonstrate previously unknown mechanisms that can lead to potentially discriminatory ad delivery, even when advertisers set their targeting parameters to be highly inclusive. This underscores the need for policymakers and platforms to carefully consider the role of the ad delivery optimization run by ad platforms themselves---and not just the targeting choices of advertisers---in preventing discrimination in digital advertising.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {199},
numpages = {30},
keywords = {online advertising, bias, ad delivery, policy, fairness}
}
@misc{probabtion,
title={ An Algorithm That Grants Freedom, or Takes It Away},
howpublished={\url{https://www.nytimes.com/2020/02/06/technology/predictive-algorithms-crime.html}},
year={2020}}
@article{CACMfairness,
author = {Krakovsky, Marina},
title = {Formalizing Fairness},
year = {2022},
issue_date = {August 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {65},
number = {8},
issn = {0001-0782},
url = {https://doi.org/10.1145/3542815},
doi = {10.1145/3542815},
abstract = {Algorithmic fairness aims to remedy issues stemming from algorithmic bias.},
journal = {Commun. ACM},
month = {jul},
pages = {11–13},
numpages = {3}
}


@article{adprediction,
  title={Automated Experiments on Ad Privacy Settings: A Tale of Opacity, Choice, and Discrimination},
  author={Amit Datta and Michael Carl Tschantz and Anupam Datta},
  journal={ArXiv},
  year={2014},
  volume={abs/1408.6491},
  url={https://api.semanticscholar.org/CorpusID:6817607}
}




@article{crimeprediction,
author = {Tim Brennan and William Dieterich and Beate Ehret},
title ={Evaluating the Predictive Validity of the Compas Risk and Needs Assessment System},
journal = {Criminal Justice and Behavior},
volume = {36},
number = {1},
pages = {21-40},
year = {2009},
doi = {10.1177/0093854808326545},

URL = { 
        https://doi.org/10.1177/0093854808326545
    
},
eprint = { 
        https://doi.org/10.1177/0093854808326545
    
}
,
    abstract = { This study examines the statistical validation of a recently developed, fourth-generation (4G) risk—need assessment system (Correctional Offender Management Profiling for Alternative Sanctions; COMPAS) that incorporates a range of theoretically relevant criminogenic factors and key factors emerging from meta-analytic studies of recidivism. COMPAS's automated scoring provides decision support for correctional agencies for placement decisions, offender management, and treatment planning. The article describes the basic features of COMPAS and then examines the predictive validity of the COMPAS risk scales by fitting Cox proportional hazards models to recidivism outcomes in a sample of presentence investigation and probation intake cases (N = 2,328). Results indicate that the predictive validities for the COMPAS recidivism risk model, as assessed by the area under the receiver operating characteristic curve (AUC), equal or exceed similar 4G instruments. The AUCs ranged from .66 to .80 for diverse offender subpopulations across three outcome criteria, with a majority of these exceeding .70. }
}





@InProceedings{facefair,
  title = 	 {Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification},
  author = 	 {Buolamwini, Joy and Gebru, Timnit},
  booktitle = 	 {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
  pages = 	 {77--91},
  year = 	 {2018},
  editor = 	 {Friedler, Sorelle A. and Wilson, Christo},
  volume = 	 {81},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--24 Feb},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf},
  url = 	 {https://proceedings.mlr.press/v81/buolamwini18a.html},
  abstract = 	 {Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist  approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6% for IJB-A and 86.2% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7%). The maximum error rate for lighter-skinned males is 0.8%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.}
}





@misc{payment,
author={Vigdor, N},
year={2019},
title={Apple card investigated after gender discrimination complaints},
publisher={The New York Times}}
@misc{jobmatching,
author={Wall, S and Schellmann, H},
year={2021},
title={LinkedIn’s job-matching
AI was biased. The company’s solution? More AI.},
publisher={MIT
Technology Review, June 23}
}






@article{
unfair2,
author = {Ziad Obermeyer  and Brian Powers  and Christine Vogeli  and Sendhil Mullainathan },
title = {Dissecting racial bias in an algorithm used to manage the health of populations},
journal = {Science},
volume = {366},
number = {6464},
pages = {447-453},
year = {2019},
doi = {10.1126/science.aax2342},
URL = {https://www.science.org/doi/abs/10.1126/science.aax2342},
eprint = {https://www.science.org/doi/pdf/10.1126/science.aax2342},
abstract = {The U.S. health care system uses commercial algorithms to guide health decisions. Obermeyer et al. find evidence of racial bias in one widely used algorithm, such that Black patients assigned the same level of risk by the algorithm are sicker than White patients (see the Perspective by Benjamin). The authors estimated that this racial bias reduces the number of Black patients identified for extra care by more than half. Bias occurs because the algorithm uses health costs as a proxy for health needs. Less money is spent on Black patients who have the same level of need, and the algorithm thus falsely concludes that Black patients are healthier than equally sick White patients. Reformulating the algorithm so that it no longer uses costs as a proxy for needs eliminates the racial bias in predicting who needs extra care. Science, this issue p. 447; see also p. 421 A health algorithm that uses health costs as a proxy for health needs leads to racial bias against Black patients. Health systems rely on commercial prediction algorithms to identify and help patients with complex health needs. We show that a widely used algorithm, typical of this industry-wide approach and affecting millions of patients, exhibits significant racial bias: At a given risk score, Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses. Remedying this disparity would increase the percentage of Black patients receiving additional help from 17.7 to 46.5\%. The bias arises because the algorithm predicts health care costs rather than illness, but unequal access to care means that we spend less money caring for Black patients than for White patients. Thus, despite health care cost appearing to be an effective proxy for health by some measures of predictive accuracy, large racial biases arise. We suggest that the choice of convenient, seemingly effective proxies for ground truth can be an important source of algorithmic bias in many contexts.}}



@inproceedings{Dwork12,
author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
title = {Fairness through Awareness},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090255},
doi = {10.1145/2090236.2090255},
abstract = {We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of "fair affirmative action," which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {214–226},
numpages = {13},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}
@inproceedings{robust,
author = {Bastani, Osbert and Ioannou, Yani and Lampropoulos, Leonidas and Vytiniotis, Dimitrios and Nori, Aditya V. and Criminisi, Antonio},
title = {Measuring Neural Net Robustness with Constraints},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Despite having high accuracy, neural nets have been shown to be susceptible to adversarial examples, where a small perturbation to an input can cause it to become mislabeled. We propose metrics for measuring the robustness of a neural net and devise a novel algorithm for approximating these metrics based on an encoding of robustness as a linear program. We show how our metrics can be used to evaluate the robustness of deep neural nets with experiments on the MNIST and CIFAR-10 datasets. Our algorithm generates more informative estimates of robustness metrics compared to estimates based on existing algorithms. Furthermore, we show how existing approaches to improving robustness "overfit" to adversarial examples generated using a specific algorithm. Finally, we show that our techniques can be used to additionally improve neural net robustness both according to the metrics that we propose, but also according to previously proposed metrics.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2621–2629},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}


@inproceedings{GMR,
author = {Goldwasser, S and Micali, S and Rackoff, C},
title = {The Knowledge Complexity of Interactive Proof-Systems},
year = {1985},
isbn = {0897911512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/22145.22178},
doi = {10.1145/22145.22178},
booktitle = {Proceedings of the Seventeenth Annual ACM Symposium on Theory of Computing},
pages = {291–304},
numpages = {14},
location = {Providence, Rhode Island, USA},
series = {STOC '85}
}

@misc{yadav2022learningtheoretic,
      title={A Learning-Theoretic Framework for Certified Auditing with Explanations}, 
      author={Chhavi Yadav and Michal Moshkovitz and Kamalika Chaudhuri},
      year={2022},
      eprint={2206.04740},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inbook{Geocert,
author = {Jordan, Matt and Lewis, Justin and Dimakis, Alexandros G.},
title = {Provable Certificates for Adversarial Examples: Fitting a Ball in the Union of Polytopes},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a novel method for computing exact pointwise robustness of deep neural networks for all convex ℓp norms. Our algorithm, GeoCert, finds the largest ℓp ball centered at an input point X0, within which the output class of a given neural network with ReLU nonlinearities remains unchanged. We relate the problem of computing pointwise robustness of these networks to that of computing the maximum norm ball with a fixed center that can be contained in a non-convex polytope. This is a challenging problem in general, however we show that there exists an efficient algorithm to compute this for polyhedral complices. Further we show that piecewise linear neural networks partition the input space into a polyhedral complex. Our algorithm has the ability to almost immediately output a nontrivial lower bound to the pointwise robustness which is iteratively improved until it ultimately becomes tight. We empirically show that our approach generates distance lower bounds that are tighter compared to prior work, under moderate time constraints.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {1261},
numpages = {11}
}


@misc{kang2022certifying,
      title={Certifying Some Distributional Fairness with Subpopulation Decomposition}, 
      author={Mintong Kang and Linyi Li and Maurice Weber and Yang Liu and Ce Zhang and Bo Li},
      year={2022},
      eprint={2205.15494},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{Boneh21,
      author = {Dan Boneh and Wilson Nguyen and Alex Ozdemir},
      title = {Efficient Functional Commitments: How to Commit to a Private Function},
      howpublished = {Cryptology ePrint Archive, Paper 2021/1342},
      year = {2021},
      note = {\url{https://eprint.iacr.org/2021/1342}},
      url = {https://eprint.iacr.org/2021/1342}
}

@inproceedings{islam2021can,
  title={Can we obtain fairness for free?},
  author={Islam, Rashidul and Pan, Shimei and Foulds, James R},
  booktitle={Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
  pages={586--596},
  year={2021}
}

@article{dwork2022distrust,
  title={Distrust of Artificial Intelligence: Sources \& Responses from Computer Science \& Law},
  author={Dwork, Cynthia and Minow, Martha},
  journal={Daedalus},
  volume={151},
  number={2},
  pages={309--321},
  year={2022},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{gupta2023sigma,
  title={SIGMA: secure GPT inference with function secret sharing},
  author={Gupta, Kanav and Jawalkar, Neha and Mukherjee, Ananta and Chandran, Nishanth and Gupta, Divya and Panwar, Ashish and Sharma, Rahul},
  journal={Cryptology ePrint Archive},
  year={2023}
}

@inproceedings{boemer2020mp2ml,
  title={MP2ML: A mixed-protocol machine learning framework for private inference},
  author={Boemer, Fabian and Cammarota, Rosario and Demmler, Daniel and Schneider, Thomas and Yalame, Hossein},
  booktitle={Proceedings of the 15th international conference on availability, reliability and security},
  pages={1--10},
  year={2020}
}

@inproceedings{juvekar2018gazelle,
  title={$\{$GAZELLE$\}$: A low latency framework for secure neural network inference},
  author={Juvekar, Chiraag and Vaikuntanathan, Vinod and Chandrakasan, Anantha},
  booktitle={27th USENIX Security Symposium (USENIX Security 18)},
  pages={1651--1669},
  year={2018}
}

@inproceedings{liu2017oblivious,
  title={Oblivious neural network predictions via minionn transformations},
  author={Liu, Jian and Juuti, Mika and Lu, Yao and Asokan, Nadarajah},
  booktitle={Proceedings of the 2017 ACM SIGSAC conference on computer and communications security},
  pages={619--631},
  year={2017}
}

@inproceedings{srinivasan2019delphi,
  title={DELPHI: A cryptographic inference service for neural networks},
  author={Srinivasan, Wenting Zheng and Akshayaram, PMRL and Ada, Popa Raluca},
  booktitle={Proc. 29th USENIX Secur. Symp},
  pages={2505--2522},
  year={2019}
}

@inproceedings{mohassel2018aby3,
  title={ABY3: A mixed protocol framework for machine learning},
  author={Mohassel, Payman and Rindal, Peter},
  booktitle={Proceedings of the 2018 ACM SIGSAC conference on computer and communications security},
  pages={35--52},
  year={2018}
}

@inproceedings{mohassel2017secureml,
  title={Secureml: A system for scalable privacy-preserving machine learning},
  author={Mohassel, Payman and Zhang, Yupeng},
  booktitle={2017 IEEE symposium on security and privacy (SP)},
  pages={19--38},
  year={2017},
  organization={IEEE}
}

@inproceedings{soares2023keeping,
  title={Keeping Up with the Language Models: Robustness-Bias Interplay in NLI Data and Models},
  author={Soares, Ioana Baldini and Yadav, Chhavi and Das, Payel and Varshney, Kush},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2023}
}

@inproceedings{hamman2023can,
  title={Can querying for bias leak protected attributes? achieving privacy with smooth sensitivity},
  author={Hamman, Faisal and Chen, Jiahao and Dutta, Sanghamitra},
  booktitle={Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
  pages={1358--1368},
  year={2023}
}

@inproceedings{casper2024black,
  title={Black-box access is insufficient for rigorous ai audits},
  author={Casper, Stephen and Ezell, Carson and Siegmann, Charlotte and Kolt, Noam and Curtis, Taylor Lynn and Bucknall, Benjamin and Haupt, Andreas and Wei, Kevin and Scheurer, J{\'e}r{\'e}my and Hobbhahn, Marius and others},
  booktitle={The 2024 ACM Conference on Fairness, Accountability, and Transparency},
  pages={2254--2272},
  year={2024}
}